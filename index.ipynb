{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Interview Cheat Sheets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>[TTS LinkedIn Candidate Guide](docs/TTS-Candidate-LinkedIn-Guide.pdf)</b> | <b>[Elements Of Programming Interviews in Python](docs/elements-of-programming-interviews-in-python.pdf)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Data Science Interview Questions](docs/ds_interview_questions.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Data Science Interview Questions](images/ds-qa.png)](docs/ds_interview_questions.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "# Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. <b> What does data science mean?</b>\n",
    "\n",
    "Data science is a \"concept to unify statistics, data analysis, informatics, and their related methods\" in order to \"understand and analyze actual phenomena\" with data. It uses techniques and theories drawn from many fields within the context of mathematics, statistics, computer science, information science, and domain knowledge. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. <b> What are the assumptions of a linear regression?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Linear regression](https://www.statology.org/linear-regression/) is a useful statistical method we can use to understand the relationship between two variables, x and y. Otherwise stated, finding the “Line of Best Fit”. We must first make sure that [four assumptions are met](https://www.statology.org/linear-regression-assumptions/):<br><br>\n",
    "- <b>Linear relationship</b>: There exists a linear relationship between the independent variable, x, and the dependent variable, y.  The points in the plot below look like they fall on roughly a straight line, which indicates that there is a linear relationship between x and y.<br><br>\n",
    "![title](images/LinReg1.jpeg)<br><br>\n",
    "\n",
    "- <b>Independence</b>: The residuals are independent. In particular, there is no correlation between consecutive residuals in time series data. Residuals are independent. This is mostly relevant when working with time series data.  The simplest way to test if this assumption is met is to look at a residual time series plot, which is a plot of residuals vs. time. Ideally, most of the residual autocorrelations should fall within the 95% confidence bands around zero.<br><br>\n",
    "\n",
    "- <b>Homoscedasticity</b>: The residuals have constant variance at every level of x.  When this is not the case, the residuals are said to suffer from [heteroscedasticity](https://en.wikipedia.org/wiki/Heteroscedasticity).<br><br>\n",
    "\n",
    "When heteroscedasticity is present in a regression analysis, the results of the analysis become hard to trust. Specifically, heteroscedasticity increases the variance of the regression coefficient estimates, but the regression model doesn’t pick up on this. This makes it much more likely for a regression model to declare that a term in the model is statistically significant, when in fact it is not.<br><br>\n",
    "\n",
    "The simplest way to detect heteroscedasticity is by creating a fitted value vs. residual plot.<br><br>\n",
    "Notice how the residuals become much more spread out as the fitted values get larger. This “cone” shape is a classic sign of heteroscedasticity:<br><br>\n",
    "![title](images/het1.jpeg)<br><br>\n",
    "\n",
    "- <b>Normality</b>: The residuals of the model are normally distributed. There are two common ways to check if this assumption is met:\n",
    "\n",
    "A Q-Q plot, short for quantile-quantile plot, is a type of plot that we can use to determine whether or not the residuals of a model follow a normal distribution. If the points on the plot roughly form a straight diagonal line, then the normality assumption is met.<br><br>\n",
    "![title](images/qq.jpeg)<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\t<b>What is the difference between factor analysis and cluster analysis</b>?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Factor analysis is an exploratory statistical technique to investigate dimensions and the factor structure\n",
    "underlying a set of variables (items) while cluster analysis is an exploratory statistical technique to group\n",
    "observations (people, things, events) into clusters or groups so that the degree of association is strong\n",
    "between members of the same cluster and weak between members of different clusters.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. <b>What is an iterator generator?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterators are containers for objects so that you can loop over the objects. In other words, you can run the \"for\" loop over the object. <br><br>\n",
    "Python generator gives us an easier way to create python iterators. This is done by defining a function but instead of the return statement returning from the function, use the \"yield\" keyword. <br><br>\n",
    "The difference is that a generator expression does not actually compute the values until they are needed. This not only leads to memory efficiency, but to computational efficiency as well! This also means that while the size of a list is limited by available memory, the size of a generator expression is unlimited!<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. <b>Write down an SQL script to return data from two tables</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INNER JOINS returns only rows where a match is found in both input tables.<br><br>\n",
    "\n",
    "```\n",
    "SELECT o.orderid, o.qty, i.itemprice, i.itemdesc\n",
    "FROM orders o\n",
    "INNER JOIN items i\n",
    "on o.itemid = i.itemid\n",
    "```\n",
    "OUTER JOINS return all rows from one table and matching rows from the second table. In cases where the join cannot find matching records from the second table, the results from the second table are displayed as NULL.  Unlike inner joins, the order in which tables are listed and joined in the FROM clause does matter, as it will determine whether you choose LEFT or RIGHT for your join.<br><br>\n",
    "```\n",
    "SELECT o.orderid, o.qty, i.itemprice, i.itemdesc\n",
    "FROM orders o\n",
    "LEFT JOIN items i\n",
    "on o.itemid = i.itemid\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. <b>Draw graphs relevant to PPC (pay-per-click) adverts and ticket purchases</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>PPC (pay-per-click)</b> advertising rates are determined using the flat-rate model or the bid-based model.\n",
    "- Flat Rate Model: In the flat rate pay-per-click model, an advertiser pays a publisher a fixed fee for each click. Publishers generally keep a list of different PPC rates that apply to different areas of their website. Note that publishers are generally open to negotiations regarding the price. A publisher is very likely to lower the fixed price if an advertiser offers a long-term or a high-value contract.<br><br>\n",
    "- Bid-Based Model: In the bid-based model, each advertiser makes a bid with a maximum amount of money they are willing to pay for an advertising spot. Then, a publisher undertakes an auction using automated tools. An auction is run whenever a visitor triggers the ad spot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Digital Marketing Dashboard:<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/digital-marketing-dashboard.png)<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ticket Sales Analysis (immersive):<br><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.8/site-packages/IPython/core/display.py:717: UserWarning: Consider using IPython.display.IFrame instead\n",
      "  warnings.warn(\"Consider using IPython.display.IFrame instead\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/8Ku12tO_X9k?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "# Youtube\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/8Ku12tO_X9k?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. <b>How can you prove an improvement you introduced to a model is actually working</b>?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. <b>Add More Data</b><br><br>\n",
    "Having more data is always a good idea. It allows the “data to tell for itself,” instead of relying on assumptions and weak correlations. Presence of more data results in better and accurate models.  For example: we do not get a choice to increase the size of training data in data science competitions.<br><br>\n",
    "\n",
    "2. <b>Treat Missing and Outlier Values</b><br><br>\n",
    "The unwanted presence of missing and outlier values in the training data often reduces the accuracy of a model or leads to a biased model. It leads to inaccurate predictions. This is because we don’t analyse the behavior and relationship with other variables correctly. So, it is important to treat missing and outlier values well.<br><br>\n",
    "     - Missing: In case of continuous variables, you can impute the missing values with mean, median, mode. For categorical variables, you can treat variables as a separate class. You can also build a model to predict the missing values. KNN imputation offers a great option to deal with missing values. To know more about these methods refer article “Methods to deal and treat missing values“.\n",
    "     - Outlier: You can delete the observations, perform transformation, binning, Imputation (Same as missing values) or you can also treat outlier values separately.<br><br>\n",
    "\n",
    "3. <b>Feature Engineering</b><br><br>\n",
    "This step helps to extract more information from existing data. New information is extracted in terms of new features. These features may have a higher ability to explain the variance in the training data. Thus, giving improved model accuracy.\n",
    "\n",
    "Feature engineering is highly influenced by hypotheses generation. Good hypothesis result in good features. That’s why, I always suggest to invest quality time in hypothesis generation.<br><br>\n",
    "\n",
    "Feature engineering process can be divided into two steps:<br><br>\n",
    "\n",
    "- - -\n",
    "\n",
    "<b>Feature Transformation</b>: There are various scenarios where feature transformation is required:\n",
    "- Changing the scale of a variable from original scale to scale between zero and one. This is known as data normalization. For example: If a data set has 1st variable in meter, 2nd in centi-meter and 3rd in kilo-meter, in such case, before applying any algorithm, we must normalize these variable in same scale.\n",
    "- Some algorithms works well with normally distributed data. Therefore, we must remove skewness of variable(s). There are methods like log, square root or inverse of the values to remove skewness.\n",
    "- Some times, creating bins of numeric data works well, since it handles the outlier values also. Numeric data can be made discrete by grouping values into bins. This is known as data discretization.\n",
    " \n",
    "- - -\n",
    "<b>Feature Creation</b>: Deriving new variable(s ) from existing variables is known as feature creation. It helps to unleash the hidden relationship of a data set. Let’s say, we want to predict the number of transactions in a store based on transaction dates. Here transaction dates may not have direct correlation with number of transaction, but if we look at the day of a week, it may have a higher correlation. In this case, the information about day of a week is hidden. We need to extract it to make the model better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. <b>Feature Selection</b>\n",
    "\n",
    "Feature Selection is a process of finding out the best subset of attributes which better explains the relationship of independent variables with target variable.  You can select the useful features based on various metrics like:\n",
    "\n",
    "- Domain Knowledge: Based on domain experience, we select feature(s) which may have higher impact on target variable.\n",
    "- Visualization: As the name suggests, it helps to visualize the relationship between variables, which makes your variable selection process easier.\n",
    "- Statistical Parameters: We also consider the p-values, information values and other statistical metrics to select right features.\n",
    "- PCA: It helps to represent training data into lower dimensional spaces, but still characterize the inherent relationships in the data. It is a type of dimensionality reduction technique. There are various methods to reduce the dimensions (features) of training data like factor analysis, low variance, higher correlation, backward/ forward feature selection and others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "5. <b>Multiple Algorithms</b>\n",
    "\n",
    "Hitting at the right machine learning algorithm is the ideal approach to achieve higher accuracy. But, it is easier said than done.  This intuition comes with experience and incessant practice. Some algorithms are better suited to a particular type of data sets than others. Hence, we should apply all relevant models and check the performance.<br><br>\n",
    "\n",
    "6. <b>Algorithm Tuning</b>\n",
    "\n",
    "We know that machine learning algorithms are driven by parameters. These parameters majorly influence the outcome of learning process. The objective of parameter tuning is to find the optimum value for each parameter to improve the accuracy of the model. To tune these parameters, you must have a good understanding of these meaning and their individual impact on model. You can repeat this process with a number of well performing models.<br><br>\n",
    "\n",
    "For example: In random forest, we have various parameters like ```max_features, number_trees, random_state, oob_score``` and others. Intuitive optimization of these parameter values will result in better and more accurate models.\n",
    "\n",
    "7. <b>Ensemble Methods</b>\n",
    "\n",
    "This is the most common approach found majorly in winning solutions of Data Science competitions. This technique simply combines the result of multiple weak models and produce better results. It is always a better idea to apply ensemble methods to improve the accuracy of your model. There are two good reasons for this: a ) They are generally more complex than traditional methods. b) The traditional methods give you a good base level from which you can improve and draw from to create your ensembles.\n",
    "\n",
    "- Bagging (Bootstrap Aggregating) involves fitting many decision trees on different samples of the same dataset and averaging the predictions.\n",
    "- Stacking involves fitting many different models types on the same data and using another model to learn how to best combine the predictions.\n",
    "- Boosting involves adding ensemble members sequentially that correct the predictions made by prior models and outputs a weighted average of the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. <b>Cross Validation</b>: To find the right answer of this question, we must use cross validation technique. Cross Validation is one of the most important concepts in data modeling. It says, try to leave a sample on which you do not train the model and test the model on this sample before finalizing the model.  This method helps us to achieve more generalized relationships.\n",
    "\n",
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CAUTION: Till here, we have seen methods which can improve the accuracy of a model. But, it is not necessary that higher accuracy models always perform better (for unseen data points). Sometimes, the improvement in model’s accuracy can be due to over-fitting too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. <b>Name several types of computer vision models</b>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different types of computer vision include \n",
    "- image segmentation\n",
    "- object detection \n",
    "- facial recognition \n",
    "- edge detection \n",
    "- pattern detection\n",
    "- image classification\n",
    "- feature matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. <b>How would you explain Random Forest to a non-technical person?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A 'decision tree' is an algorithm that tries to split up the data based on a series of (usually binary) questions.<br><br> \n",
    "<b>Example</b>: If we want to learn about the set of all dogs, we could ask \"Big or small\", \"Long hair or short hair\", \"pure or mutt\" etc.  Basically, this is like a game of 20 questions where the algorithm tries to narrow down the scope of possibilities to something more specific. For each question, the decision tree tries to find the best cutoff between \"yes\" and \"no\" (e.g., \"big\" or \"small\" for our dog example) based on either an entropy or 'gini' score.  The results of this sequence of questions can be visualized as a tree, where node represents a question and each branch represents an answer.<br><br>\n",
    "Now a given tree is determined by \n",
    "- which questions to ask and in what order  \n",
    "- the sample of data that the tree is trained on\n",
    "\n",
    "<b>Preventing over-fitting</b>: A random tree is a tree where either the questions or the data are selected randomly.<br><br>   \n",
    "\n",
    "A random forest is a set of random trees, where the final result is taken to be an average (regression) or a 'vote' (classification) from among the individual trees.<br><br>\n",
    "\n",
    "<b>Algorithms:</b> Different decision tree algorithms utilize different impurity metrics: \n",
    "- - -\n",
    "- CART uses Gini; \n",
    "CART is more sensitive to outliers at the target variable (Y) than the predictors (X). It is calculated by summing the split improvement score for each variable across all splits in a tree. Random Forest creates multiple CART trees based on \"bootstrapped\" samples of data and then combines the predictions.\n",
    "- - -\n",
    "- ID3 and C4. 5 use Entropy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. <b>What is and how to determine a Gini coefficient?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gini coefficient is a statistical measure used to calculate inequality within a nation. It does so by calculating the wealth distribution between members of the population. The Gini coefficient can be calculated using the formula: \n",
    "\n",
    "```\n",
    "Gini Coefficient = A / (A + B)\n",
    "```\n",
    "where A is the area above the Lorenz Curve and B is the area below the Lorenz Curve.<br>\n",
    "![title](images/gini.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Explain K-means.\n",
    "11. What kind of RDBMS software do you have experience with? What about non-relational databases?\n",
    "14. What is the difference between SQL, MySQL and SQLServer?\n",
    "\n",
    "16. Give examples where a false negative is more important than a false positive, and vice versa.\n",
    "17. What is alogistic regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. <b>How would you start cleaning a big dataset</b>?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "Step 1: <b>Remove duplicate or irrelevant observations</b><br><br>\n",
    "Remove unwanted observations from your dataset, including duplicate observations or irrelevant observations. Duplicate observations will happen most often during data collection. When you combine data sets from multiple places, scrape data, or receive data from clients or multiple departments, there are opportunities to create duplicate data. De-duplication is one of the largest areas to be considered in this process. Irrelevant observations are when you notice observations that do not fit into the specific problem you are trying to analyze. For example, if you want to analyze data regarding millennial customers, but your dataset includes older generations, you might remove those irrelevant observations. This can make analysis more efficient and minimize distraction from your primary target—as well as creating a more manageable and more performant dataset.\n",
    "\n",
    "- - -\n",
    "Step 2: <b>Fix structural errors</b><br><br>\n",
    "Structural errors are when you measure or transfer data and notice strange naming conventions, typos, or incorrect capitalization. These inconsistencies can cause mislabeled categories or classes. For example, you may find “N/A” and “Not Applicable” both appear, but they should be analyzed as the same category.\n",
    "- - -\n",
    "Step 3: <b>Filter unwanted outliers</b><br><br>\n",
    "Often, there will be one-off observations where, at a glance, they do not appear to fit within the data you are analyzing. If you have a legitimate reason to remove an outlier, like improper data-entry, doing so will help the performance of the data you are working with. However, sometimes it is the appearance of an outlier that will prove a theory you are working on. Remember: just because an outlier exists, doesn’t mean it is incorrect. This step is needed to determine the validity of that number. If an outlier proves to be irrelevant for analysis or is a mistake, consider removing it.\n",
    "- - -\n",
    "Step 4: <b>Handle missing data</b><br><br>\n",
    "You can’t ignore missing data because many algorithms will not accept missing values. There are a couple of ways to deal with missing data. Neither is optimal, but both can be considered.\n",
    "\n",
    "As a first option, you can drop observations that have missing values, but doing this will drop or lose information, so be mindful of this before you remove it.\n",
    "As a second option, you can input missing values based on other observations; again, there is an opportunity to lose integrity of the data because you may be operating from assumptions and not actual observations.\n",
    "As a third option, you might alter the way the data is used to effectively navigate null values.\n",
    "\n",
    "- - -\n",
    "Step 5: <b>Validate and QA</b><br><br>\n",
    "At the end of the data cleaning process, you should be able to answer these questions as a part of basic validation:\n",
    "\n",
    "- Does the data make sense?\n",
    "- Does the data follow the appropriate rules for its field?\n",
    "- Does it prove or disprove your working theory, or bring any insight to light?\n",
    "- Can you find trends in the data to help you form your next theory?\n",
    "- If not, is that because of a data quality issue?\n",
    "- - -\n",
    "- False conclusions because of incorrect or “dirty” data can inform poor business strategy and decision-making. - - False conclusions can lead to an embarrassing moment in a reporting meeting when you realize your data doesn’t stand up to scrutiny. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. <b>What is over fitting and how to fix it?</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting occurs when you achieve a good fit of your model on the training data, while it does not generalize well on new, unseen data.  Translated: the model learned patterns specific to the training data, which are irrelevant in other data.<br><br>\n",
    "One can identify overfitting by looking at validation metrics, like loss or accuracy.<br><br>\n",
    "<b>Best way to fix it is - get more training data.</b><br><br>\n",
    "Or other ways to handle overfitting:\n",
    "- Reduce the network’s capacity by removing layers or reducing the number of elements in the hidden layers\n",
    "- Apply regularization, which comes down to adding a cost to the loss function for large weights\n",
    "- Use Dropout layers, which will randomly remove certain features by setting them to zero<br><br>The goal is to reduce overfitting by lowering the capacity of the model to memorize the training data.<br><br>\n",
    "\n",
    "<b>Example</b>: Determing airline passengers sentiment on Twitter [Data](https://www.kaggle.com/crowdflower/twitter-airline-sentiment) | [Code](https://towardsdatascience.com/handling-overfitting-in-deep-learning-models-c760ee047c6e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16. <b>[Machine Learning](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/): Supervised learning versus unsupervised learning.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "SUPERVISED LEARNING is a machine learning approach that’s defined by its use of labeled datasets. These datasets are designed to train or “supervise” algorithms into classifying data or predicting outcomes accurately. Using labeled inputs and outputs, the model can measure its accuracy and learn over time.<br><br>\n",
    "Supervised learning can be separated into two types of problems when data mining: classification and regression:\n",
    "- CLASSIFICATION problems use an algorithm to accurately assign test data into specific categories, such as separating apples from oranges. Or, in the real world, supervised learning algorithms can be used to classify spam in a separate folder from your inbox. Linear classifiers, support vector machines, decision trees and random forest are all common types of classification algorithms.\n",
    "- REGRESSION is another type of supervised learning method that uses an algorithm to understand the relationship between dependent and independent variables. Regression models are helpful for predicting numerical values based on different data points, such as sales revenue projections for a given business. Some popular regression algorithms are linear regression, logistic regression and polynomial regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "UNSUPERVISED LEARNING uses machine learning algorithms to analyze and cluster unlabeled data sets. These algorithms discover hidden patterns in data without the need for human intervention (hence, they are “unsupervised”).<br><br>\n",
    "\n",
    "Unsupervised learning models are used for three main tasks: clustering, association and dimensionality reduction:<br><br>\n",
    "- CLUSTERING is a data mining technique for grouping unlabeled data based on their similarities or differences. For example, K-means clustering algorithms assign similar data points into groups, where the K value represents the size of the grouping and granularity. This technique is helpful for market segmentation, image compression, etc.\n",
    "- ASSOCIATION is another type of unsupervised learning method that uses different rules to find relationships between variables in a given dataset. These methods are frequently used for market basket analysis and recommendation engines, along the lines of “Customers Who Bought This Item Also Bought” recommendations.\n",
    "- DIMENSIONALITY REDUCTION is a learning technique used when the number of features  (or dimensions) in a given dataset is too high. It reduces the number of data inputs to a manageable size while also preserving the data integrity. Often, this technique is used in the preprocessing data stage, such as when autoencoders remove noise from visual data to improve picture quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Main Difference:</b> The main distinction between the two approaches is the use of LABELED DATASETS. To put it simply, supervised learning uses labeled input and output data, while an unsupervised learning algorithm does not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17. <b>State some biases that you are likely to encounter when cleaning a database.<b><br><br>\n",
    "\n",
    "- - -\n",
    "    \n",
    "<b>SAMPLE BIAS</b><br>\n",
    "\n",
    "Happens when the collected data doesn’t accurately represent the environment the program is expected to run into. There is no algorithm that can be trained on the entire universe of data, rather than a subset that is carefully chosen. There’s a science of choosing this subset that is both large enough and representative enough to mitigate sample bias.<br><br>\n",
    "<b>Example: Security cameras</b><br>\n",
    "\n",
    "If your goal is to create a model that can operate security cameras at daytime and nighttime, but train it on nighttime data only. You’ve introduced sample bias into your model.<br>\n",
    "    \n",
    "SAMPLE BIAS can be reduced or eliminated by:\n",
    "- Training your model on both daytime and nighttime.\n",
    "- Covering all the cases you expect your model to be exposed to. This can be done by examining the domain of each feature and make sure we have balanced evenly-distributed data covering all of it. Otherwise, you’ll be faced by erroneous results and outputs the don’t make sense will be produced.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "<b>EXCLUSION BIAS</b><br><br>\n",
    "Happens as a result of excluding some feature(s) from our dataset usually under the umbrella of cleaning our data.\n",
    "We delete some feature(s) thinking that they’re irrelevant to our labels/outputs based on pre-existing beliefs.<br><br>\n",
    "<b>Example: Titanic Survival prediction</b><br>   \n",
    "In the famous titanic problem where we predict who survived and who didn’t. One might disregard the passenger id of the travelers as they might think that it is completely irrelevant to whether they survived or not.  Little did they know that Titanic passengers were assigned rooms according to their passenger id. The smaller the id number the closer their assigned rooms are to the lifeboats which made those people able to get to lifeboats faster than those who were deep in the center of the Titanic. Thus, resulting in a lesser ratio of survival as the id increases.<br><br>\n",
    "EXCLUSION BIAS can be reduced or eliminated by:\n",
    "- Investigate before discarding feature(s) by doing sufficient analysis on them.\n",
    "- Ask a colleague to look into the feature(s) you’re considering to discard, afresh pair of eyes will definitely help.\n",
    "- If you’re low on time/resources and need to cut your dataset size by discarding feature(s). Before deleting any, make sure to search the relation between this feature and your label. Most probably you’ll find similar solutions, investigate whether they’ve taken into account similar features and decide then.\n",
    "- Remember humans are subject to bias. There are tools that can help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "<b>OBSERVER BIAS (aka experimenter bias)</b><br><br>\n",
    "\n",
    "The tendency to see what we expect to see, or what we want to see. When a researcher studies a certain group, they usually come to an experiment with prior knowledge and subjective feelings about the group being studied. In other words, they come to the table with conscious or unconscious prejudices.<br>\n",
    "\n",
    "<b>Example: Is Intelligence influenced by status? — The Burt Affair</b><br><br>\n",
    "One famous example of observer bias is the work of Cyril Burt, a psychologist best known for his work on the heritability of IQ. He thought that children from families with low socioeconomic status (i.e. working class children) were also more likely to have lower intelligence, compared to children from higher socioeconomic statuses. His allegedly scientific approach to intelligence testing was revolutionary and allegedly proved that children from the working classes were in general, less intelligent. This led to the creation of a two-tier educational system in England in 1960s which sent middle and upper-class children to elite schools and working-class children to less desirable schools.<br><br>\n",
    "Burt’s research was later of course debunked and it was concluded he falsified data. It is now accepted that intelligence is not hereditary.<br><br>\n",
    "OBSERVER BIAS can be reduced or eliminated by:\n",
    "- Ensuring that observers (people conducting experiments) are well trained.\n",
    "- Screening observers for potential biases.\n",
    "- Having clear rules and procedures in place for the experiment.\n",
    "- Making sure behaviors are clearly defined.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "\n",
    "<b>PREJUDICE BIAS</b><br><br>\n",
    "Happens as a result of cultural influences or stereotypes. When things that we don’t like in our reality like judging by appearances, social class, status, gender and much more is not fixed in our machine learning model. When this model applies the same stereotyping that exists in real life due to prejudiced data it is fed.<br><br>\n",
    "<b>Example: A computer vision program that detects people at work</b><br><br>\n",
    "If your goal is to detect people at work. Your model has been fed to thousands of training data where men are coding and women are cooking. The algorithm is likely to learn that coders are men and women are chefs. Which is wrong since women can code and men can cook.<br><br>\n",
    "The problem here is that the data is consciously or unconsciously reflecting stereotypes.<br>\n",
    "Prejudice bias can be reduced or eliminated by:\n",
    "- Ignoring the statistical relationship between gender and occupation.\n",
    "- Exposing the algorithm to a more even-handed distribution of examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "<b>MEASUREMENT BIAS</b><br><br>\n",
    "Systematic value distortion happens when there’s an issue with the device used to observe or measure. This kind of bias tends to skew the data in a particular direction.<br><br>\n",
    "<b>Example: Shooting images data with a camera that increases the brightness.</b><br><br>\n",
    "This messed up measurement tool failed to replicate the environment on which the model will operate, in other words, it messed up its training data that it no longer represents real data that it will work on when it’s launched.<br><br>\n",
    "This kind of bias can’t be avoided simply by collecting more data.<br><br>\n",
    "MEASUREMENT BIAS can be reduced or eliminated by:\n",
    "- Having multiple measuring devices.\n",
    "- Hiring humans who are trained to compare the output of these devices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "### Bias Testing Tools\n",
    "Bias Testing in your product development cycle\n",
    "1. [FairML](https://github.com/adebayoj/fairml)\n",
    "A ToolBox for diagnosing bias in predictive modeling. It audits them & determines the significance of the inputs. \n",
    "\n",
    "2. [LIME](https://github.com/marcotcr/lime)\n",
    "[Local Interpretable Model-Agnostic Explanations (LIME)](https://www.oreilly.com/learning/introduction-to-local-interpretable-model-agnostic-explanations-lime). Many machine learning models are black boxes, understanding the rationale behind the model’s predictions would certainly help users decide when to trust or not to trust their predictions.  Lime's purpose is to explain what machine learning classifiers (or models) are doing.\n",
    "\n",
    "<b>Example: Predicting the flu</b><br><br>\n",
    "A model predicts that a certain patient has the flu. The prediction is then explained by an “explainer” that highlights the symptoms that are most important to the model. With this information about the rationale behind the model, the doctor is now empowered to trust the model—or not.\n",
    "![title](images/figure1.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>How it works</b>: generate a data set of perturbed instances by turning some of the interpretable components “off” (in this case, making them gray). For each perturbed instance, we get the probability that a tree frog is in the image according to the model. \n",
    "![title](images/figure2.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18. <b>What is marketing automation</b>?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Marketing automation is the process by which software is used to automate conventional marketing processes.  Marketing automation helped companies segment customers, launch multichannel marketing campaigns, and provide personalized information for customers., based on their specific activities. In this way, users activity (or lack thereof) triggers a personal message that is customized to the user in their preferred platform. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "19. <b>What is root cause analysis</b>?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In science and engineering, root cause analysis is a method of problem solving used for identifying the root causes of faults or problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "# Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>What is the difference between false positive and false negative?</b><br><br>\n",
    "A false positive is when a scientist determines something is true when it is actually false (also called a type I error). A false positive is a “false alarm.” <br><br>A false negative is saying something is false when it is actually true (also called a type II error). <br><br><b>Examples:</b><br><br>\n",
    "- Airport Security: a \"false positive\" is when ordinary items such as keys or coins get mistaken for weapons\n",
    "- Quality Control: a \"false positive\" is when a good quality item gets rejected, and a \"false negative\" is when a poor quality item gets accepted. (A \"positive\" result means there IS a defect.)\n",
    "- COVID Test: a \"false negative\" - there's a chance that your COVID-19 diagnostic test could return a false-negative result. This means that the test didn't detect the virus, even though you actually are infected with it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>What is the null hypothesis and how do we state it?</b><br><br>\n",
    "To write a null hypothesis, first start by asking a question. Rephrase that question in a form that assumes no relationship between the variables. In other words, assume a treatment has no effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>How would you explain a linear regression to a business executive?</b><br><br>\n",
    "Linear regression models are used to show or predict the relationship between two variables or factors. The factor that is being predicted (the factor that the equation solves for) is called the dependent variable. The factors that are used to predict the value of the dependent variable are called the independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>Tell me what heteroskedasticity is and how to solve it.</b><br><br>\n",
    "\n",
    "Its compliment is homoscedastic, or a sequence of random variables is homoscedastic if all its random variables have the same finite variance. See image below.\n",
    "![title](images/homo1.png)\n",
    "\n",
    "In simple terms, heteroscedasticity is any set of data that isn't homescedastic.  More technically, it refers to data with unequal variability (scatter) across a set of second, predictor variables. Heteroscedastic data tends to follow a cone shape on a scatter graph.<br><br>\n",
    "When heteroscedasticity is present in a regression analysis, the results of the analysis become hard to trust. This makes it much more likely for a regression model to declare that a term in the model is statistically significant, when in fact it is not.<br><br>\n",
    "The simplest way to detect heteroscedasticity is with a fitted value vs. residual plot. Once you fit a regression line to a set of data, you can then create a scatterplot that shows the fitted values of the model vs. the residuals of those fitted values.<br><br>\n",
    "![title](images/het2.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the residuals become much more spread out as the fitted values get larger. This “cone” shape is a telltale sign of heteroscedasticity.<br><br>\n",
    "<b>Example 1</b>: Consider a dataset that includes the annual income and expenses of 100,000 people across the United States. For individuals with lower incomes, there will be lower variability in the corresponding expenses since these individuals likely only have enough money to pay for the necessities. For individuals with higher incomes, there will be higher variability in the corresponding expenses since these individuals have more money to spend if they choose to.<br><br>\n",
    "<b>Example 2</b>: Consider a dataset that includes the populations and the count of flower shops in 1,000 different cities across the United States. For cities with small populations, it may be common for only one or two flower shops to be present. But in cities with larger populations, there will be a much greater variability in the number of flower shops. These cities may have anywhere between 10 to 100 shops. This means when we create a regression analysis and use population to predict number of flower shops, there will inherently be greater variability in the residuals for the cities with higher populations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Why do we log a variable?</b> When logs are applied, the distributions are better behaved. Taking logs also reduces the extrema in the Page 7 data, and curtails the effects of outliers. We often see economic variables measured in dol- lars in log form, while variables measured in units of time, or interest rates, are often left in levels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How To Solve Heteroskedasticity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Transform the dependent variable<br><br>\n",
    "One common transformation is to simply take the log of the dependent variable. For example, if we are using population size (independent variable) to predict the number of flower shops in a city (dependent variable), we may instead try to use population size to predict the log of the number of flower shops in a city.<br><br>\n",
    "\n",
    "2. Redefine the dependent variable.<br><br>\n",
    "For example, instead of using the population size to predict the number of flower shops in a city, we may instead use population size to predict the number of flower shops per capita. this reduces the variability that naturally occurs among larger populations since we’re measuring the number of flower shops per person, rather than the sheer amount of flower shops.<br><br>\n",
    "\n",
    "3. Use weighted regression<br><br>\n",
    "Another way to fix heteroscedasticity is to use weighted regression. This type of regression assigns a weight to each data point based on the variance of its fitted value. Essentially, this gives small weights to data points that have higher variances, which shrinks their squared residuals. When the proper weights are used, this can eliminate the problem of heteroscedasticity.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "# Coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Python](https://www.python.org/), [R](https://www.r-project.org/), [SAS](https://sas.com) (optional) and [SQL](https://www.tutorialspoint.com/sql/sql-overview.htm) are the bread-and-butter programming languages in data science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
